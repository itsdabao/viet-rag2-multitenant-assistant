# Force provider selection (optional)
# LLM_PROVIDER=groq | openai_compat | llama_cpp | gemini | openai | none
LLM_PROVIDER=groq

# Primary (recommended): Groq (OpenAI-compatible)
GROQ_API_KEY=your_groq_api_key_here
GROQ_MODEL=meta-llama/llama-4-scout-17b-16e-instruct
GROQ_BASE_URL=https://api.groq.com/openai/v1

# Local/Remote OpenAI-compatible (recommended for local models)
# Example base_url:
# - LM Studio: http://localhost:1234/v1
# - vLLM:      http://localhost:8001/v1
# - Ollama:    http://localhost:11434/v1 (if OpenAI-compat enabled)
# Usage:
#   LLM_PROVIDER=openai_compat
#   OPENAI_COMPAT_BASE_URL=http://localhost:1234/v1
#   OPENAI_COMPAT_MODEL=your-local-model-id
# Optional (many local servers ignore this, but our client always sends Authorization header):
#   OPENAI_COMPAT_API_KEY=local
# OPENAI_COMPAT_BASE_URL=http://localhost:1234/v1
# OPENAI_COMPAT_MODEL=local-model
# OPENAI_COMPAT_API_KEY=local

# llama-cpp-python (in-process GGUF) - dùng khi bạn load GGUF trực tiếp bằng llama_cpp.Llama
# LLM_PROVIDER=llama_cpp
# LLAMA_CPP_MODEL_PATH=models\\qwen2.5-3b-instruct-q4_k_m.gguf
# LLAMA_CPP_N_GPU_LAYERS=-1
# LLAMA_CPP_N_CTX=2048
# Optional:
# LLAMA_CPP_CHAT_FORMAT=chatml
# LLAMA_CPP_N_THREADS=0
# LLAMA_CPP_TEMPERATURE=0.2
# LLAMA_CPP_MAX_TOKENS=1024
# LLAMA_CPP_VERBOSE=0

# Optional: LlamaParse (Modern Ingestion cho PDF phức tạp)
# LLAMA_CLOUD_API_KEY=your_llama_cloud_key_here

# Optional fallback: Gemini
# GOOGLE_API_KEY=your_google_api_key_here
# GEMINI_MODEL=gemini-2.5-flash-lite

# Optional fallback: OpenAI
# OPENAI_API_KEY=your_openai_api_key_here
# OPENAI_MODEL=gpt-4o-mini

# --- Postgres (Day 6-7 persistent memory) ---
# For local dev:
# DATABASE_URL=postgresql+psycopg2://admin:123@localhost:5432/agent_memory
# If running app inside another Docker container on the same compose network:
# DATABASE_URL=postgresql+psycopg2://admin:123@db:5432/agent_memory
# Or SQLite (quick local dev, no Postgres needed):
# DATABASE_URL=sqlite:///./data/agent.db
DATABASE_URL=
# Optional (defaults to "chat_sessions")
CHAT_SESSIONS_TABLE=chat_sessions

# Memory controls (apply to rolling_summary + last N turns)
MEMORY_ENABLED=1
MEMORY_LAST_TURNS=6
MEMORY_BUDGET_TOKENS=1000
MEMORY_SUMMARY_ENABLED=1
# Optional (defaults to 350)
MEMORY_SUMMARY_MAX_OUTPUT_TOKENS=350

# Prompt history (non-DB fallback history list)
# Optional (defaults to 12 messages ~= 6 turns)
HISTORY_MAX_TURNS=12

# --- Firebase (protected /chat endpoint) ---
# Path to Firebase Admin SDK service account JSON.
# If omitted, backend tries `service_account.json` at repo root (not recommended to commit).
FIREBASE_SERVICE_ACCOUNT_PATH=

# --- Owner Console (local-first) ---
# Reserved for Owner Console JWT login (Phase after Admin Dashboard).
OWNER_USERNAME=owner
OWNER_PASSWORD=owner_password_here
JWT_SECRET=change_me_to_a_long_random_string
# Optional (minutes), default 1440 (1 day)
JWT_EXPIRE_MIN=1440

# --- Debug (default OFF) ---
# WARNING: turning these on can log prompts/contexts (may include PII).
DEBUG_VERBOSE=0
DEBUG_SHOW_PROMPT=0
DEBUG_TOPN_PRINT=3
